A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine
Cao, L. J., et al. "A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine." Neurocomputing 55.1-2 (2003): 321-336.

This paper explores the efficacy of using principal component analysis (PCA), kernel principal component analysis (KPCA), and independent component analysis (ICA) as methods for feature extraction prior to their application in a support vector machine (SVM) algorithm. These three methods of feature extraction serve to reduce the dimensionality of the dataspace, with the goal of decreasing the contribution of irrelevant or correlated attributes. Principal component analysis calculates the eigen vectors of the covariance matrix to produce a set of uncorrelated vectors. KPCA generalizes the kernel method into PCA and allows for nonlinear PCA. Finally, ICA is another linear transformation which attempts to identify statistically independent components, and is widely used in the context of blind source separation. 

Following feature extraction with one of the three methods, data was then used as input for an SVM algorithm. These combined algorithms were then implemented on a diverse array of datasets, including the Sunspot dataset, Santa Fe A dataset, as well as financial data from 5 sets of financial futures obtained from the Chicago Mercantile Market. Performance for of each algorithm was measured using the residual mean square error (RMSE). Parameters for each strategy were optimized until the minimum RMSE could be obtained, and compared across algorithms. Additionally, RMSE was computed using SVM without dimensionality reduction. The authors compared resulting RMSEs, with lower values indicating increased algorithm performance. In review of the data, the authors show that using any feature extraction algorithm yielded lower RMSE (and thus superior performance) than applying SVM to the dataset alone. Additionally, KPCA+SVM consistently outperformed ICA+SVM, which in turn outperformed PCA+SVM. However, KPCA was also more computationally expensive, requiring orders of magnitude more CPU use time compared to the other methods of feature extraction. In concluding remarks, the authors speculate that KPCA and ICA can use higher order information from the original input, whereas the covariance matrix used in PCA uses second order information.
